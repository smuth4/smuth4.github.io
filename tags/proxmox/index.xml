<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Proxmox on cat /dev/null &gt; /proc/mind</title>
    <link>https://smuth.me/tags/proxmox/</link>
    <description>Recent content in Proxmox on cat /dev/null &gt; /proc/mind</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://smuth.me/tags/proxmox/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Enabling NFS mounts in Proxmox 5.2 LXC containers</title>
      <link>https://smuth.me/post/enabling-nfs-proxmox-lxc/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://smuth.me/post/enabling-nfs-proxmox-lxc/</guid>
      <description>Normally, Proxmox doesn&amp;rsquo;t allow mounting NFS mounts directly in containers due to security concerns. In the past it was possible to modify apparmor profiles directly in order to allow it, as seen here and here. However, as of this commit, that method is no longer an option, due to the apparmor profiles now being generated dynamically when the containers are started (you can view these profiles at /var/lib/lxc/${CID}/apparmor/lxc-${CID}_&amp;lt;-var-lib-lxc&amp;gt;. Instead, modifying apparmor directly is no longer necessary, as you can now add the undocumented feature setting to the pct.</description>
    </item>
    
    <item>
      <title>Enabling jumbo frames on Proxmox 4</title>
      <link>https://smuth.me/post/proxmox-4-enabling-jumbo-frames/</link>
      <pubDate>Sun, 02 Oct 2016 15:58:01 +0000</pubDate>
      
      <guid>https://smuth.me/post/proxmox-4-enabling-jumbo-frames/</guid>
      <description>While turning on jumbo frames on a basic interface is straightforward (ip link set eth0 mtu 9000), Proxmox&amp;rsquo;s use of bridges to connect VMs makes things much more interesting. To start with, all interfaces connected to the bridge must have their MTUs upgraded first, otherwise it will give you an unhelpful error. Note that interfaces connected to the bridge include those of running containers/VMs.1
# ip a | grep mtu 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc mq master vmbr0 state UP group default qlen 1000 3: vmbr0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 4: veth102i0@if7: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master vmbr0 state UP group default qlen 1000 # ip link set dev vmbr0 mtu 9000 RTNETLINK answers: Invalid argument # ip link set dev eth0 mtu 9000 # ip link set dev veth102i0 mtu 9000 # ip link set dev vmbr0 mtu 9000  Note that setting the MTU on vmbr0 is technically unnecessary, since bridges inherit the smallest MTU of the slaved devices2.</description>
    </item>
    
  </channel>
</rss>